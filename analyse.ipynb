{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates the process of training a reward model based on human preferences and evaluating its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Check if directories exist, create if not\n",
    "os.makedirs('q2_reward', exist_ok=True)\n",
    "os.makedirs('q2_reward/reward_model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate and Rank Answers\n",
    "\n",
    "First, we need to generate answers for our prompts and rank them according to our preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if answers.csv exists\n",
    "if os.path.exists('q2_reward/answers.csv'):\n",
    "    print('Loading existing answers.csv')\n",
    "    df = pd.read_csv('q2_reward/answers.csv')\n",
    "    display(df.head())\n",
    "else:\n",
    "    print('Please run generate_answers.py first to create the answers.csv file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset for Reward Model Training\n",
    "\n",
    "We'll convert our ranked answers into pairs for comparison training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_reward_dataset(df):\n",
    "    # Create pairs of answers for comparison\n",
    "    pairs = []\n",
    "    prompts = df['prompt'].unique()\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        prompt_df = df[df['prompt'] == prompt]\n",
    "        answers = prompt_df['answer'].tolist()\n",
    "        ranks = prompt_df['rank'].tolist()\n",
    "        \n",
    "        # Create pairs where better ranked answers are chosen over worse ranked ones\n",
    "        for i in range(len(answers)):\n",
    "            for j in range(i+1, len(answers)):\n",
    "                if ranks[i] < ranks[j]:  # Lower rank is better\n",
    "                    pairs.append({\n",
    "                        \"prompt\": prompt,\n",
    "                        \"chosen\": answers[i],\n",
    "                        \"rejected\": answers[j]\n",
    "                    })\n",
    "                else:\n",
    "                    pairs.append({\n",
    "                        \"prompt\": prompt,\n",
    "                        \"chosen\": answers[j],\n",
    "                        \"rejected\": answers[i]\n",
    "                    })\n",
    "    \n",
    "    return Dataset.from_pandas(pd.DataFrame(pairs))\n",
    "\n",
    "# Prepare the dataset if answers.csv exists\n",
    "if 'df' in locals():\n",
    "    dataset = prepare_reward_dataset(df)\n",
    "    print(f'Created dataset with {len(dataset)} comparison pairs')\n",
    "    print('\nExample pair:')\n",
    "    print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Reward Model\n",
    "\n",
    "We'll use HuggingFace's TRL library to train our reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This cell demonstrates the training process\n",
    "# Note: The actual training is done in train_reward_model.py\n",
    "\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "# Code snippet showing how the training is configured\n",
    "print(\"Training Configuration Sample:\")\n",
    "print(\"\"\"\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"q2_reward/reward_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    max_length=512,\n",
    "    max_steps=100,  # Train for 100 steps as required\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"]\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Reward Model\n",
    "\n",
    "Let's analyze the results of our reward model on new answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if evaluation results exist\n",
    "if os.path.exists('q2_reward/evaluation_results.csv'):\n",
    "    eval_df = pd.read_csv('q2_reward/evaluation_results.csv')\n",
    "    print('Loaded evaluation results')\n",
    "    display(eval_df.head())\n",
    "else:\n",
    "    print('Please run evaluate_reward_model.py first to create the evaluation results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the reward scores\n",
    "if 'eval_df' in locals():\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    prompts = eval_df['prompt'].unique()\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        prompt_results = eval_df[eval_df['prompt'] == prompt].sort_values('score', ascending=False)\n",
    "        \n",
    "        plt.subplot(len(prompts), 1, i+1)\n",
    "        bars = plt.bar(\n",
    "            range(len(prompt_results)), \n",
    "            prompt_results['score'],\n",
    "            color='skyblue'\n",
    "        )\n",
    "        \n",
    "        # Add temperature labels\n",
    "        plt.xticks(\n",
    "            range(len(prompt_results)),\n",
    "            [f\"Temp={t}\" for t in prompt_results['temperature']]\n",
    "        )\n",
    "        \n",
    "        # Add score values on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width()/2.,\n",
    "                height + 0.02,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=9\n",
    "            )\n",
    "        \n",
    "        plt.title(f\"Prompt: {prompt[:50]}...\")\n",
    "        plt.ylabel(\"Reward Score\")\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"q2_reward/reward_scores_analysis.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis\n",
    "\n",
    "Let's analyze if higher reward scores correlate with better answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if 'eval_df' in locals():\n",
    "    # For each prompt, check if there's a correlation between temperature and score\n",
    "    prompts = eval_df['prompt'].unique()\n",
    "    \n",
    "    print(\"Correlation Analysis:\n\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        prompt_results = eval_df[eval_df['prompt'] == prompt]\n",
    "        correlation = np.corrcoef(prompt_results['temperature'], prompt_results['score'])[0, 1]\n",
    "        \n",
    "        print(f\"Prompt: {prompt[:50]}...\n\")\n",
    "        print(f\"Correlation between temperature and score: {correlation:.4f}\")\n",
    "        \n",
    "        # Display the answers sorted by score\n",
    "        sorted_results = prompt_results.sort_values('score', ascending=False)\n",
    "        print(\"\nAnswers ranked by reward score (highest to lowest):\n\")\n",
    "        \n",
    "        for i, (_, row) in enumerate(sorted_results.iterrows()):\n",
    "            print(f\"Rank {i+1} (Score: {row['score']:.4f}, Temp: {row['temperature']}):\n\")\n",
    "            print(f\"{row['answer'][:100]}...\n\")\n",
    "        \n",
    "        print(\"\n" + "-\"*80 + \"\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the process of training a reward model based on human preferences. The key steps were:\n",
    "\n",
    "1. Generating multiple answers for each prompt\n",
    "2. Ranking the answers according to human preferences\n",
    "3. Training a reward model using the TRL library\n",
    "4. Evaluating the reward model on new answers\n",
    "5. Analyzing if higher reward scores correlate with better answers\n",
    "\n",
    "The reward model can be used to guide the generation of better responses in a reinforcement learning from human feedback (RLHF) pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}